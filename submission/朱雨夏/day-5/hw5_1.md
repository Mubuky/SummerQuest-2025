# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
在 Column Parallel 策略中，权重矩阵W1∈R^{d×4d}被沿着列（第二个维度）切分到 4 个 rank 上。因此每个 rank 上的权重矩阵的 shape 是：[d,d]=[1024,1024]

#### 1.1.2
B * S * D
输入张量 X∈R^{B×S×d}在 Column Parallel 策略中不需要切分，因此每个 rank 上的输入张量的 shape 是：[B,S,d]=[8,128,1024]

#### 1.1.3
每个 rank 上的本地输出Yi通过 X*W1^(i)计算得到，因此其 shape 是：[B, S, d] = [8,128,1024]
拼接操作需要跨 rank 的通信,通过All gather得到

### 1.2


#### 1.2.1
Row Parallel策略中，权重矩阵W1∈R^{4d×d}被沿着行（第一个维度）切分到 4 个 rank 上。因此每个 rank 上的权重矩阵的 shape 是：[d,d]=[1024,1024]

#### 1.2.2
B * S * D
由于Linear1采用Column Parallel，其输出 Y∈R^{B×S×4d}需要被切分以匹配Linear2的Row Parallel 输入,因此每个rank接受的输入张量是 Y沿最后一个维度切分后的部分Y^(i),shape是：[B, S, d] = [8,128,1024]

#### 1.2.3
每个rank上的本地输出Zi是通过Y^(i)*W2^(i)计算得到的，因此其shape是:[B, S, d] = [8,128,1024]
完整的Z通过对各个 rank 上的Zi跨 rank 求和得到,即All Reduce

## 2 通信分析

### 2.1

#### 2.1.1
不需要通信，计算得到的Yi直接用于下一步

#### 2.1.2
需要通信
当前层完成反向传递到下一层时，需要做一次All reduce，即反向传播计算的X的总梯度∂L/∂X是所有回传的梯度∂L/∂Xi之和

### 2.2

#### 2.2.1
需要，因为行切分，做完后需要一次All recude，通信量是2*(P-1)*B*S*D=2×8×128×1024
#### 2.2.2
不需要，此时每个rank都有完整的Z，计算输入的梯度时 ∂L/∂Y_i = (∂L/∂Z) * (∂Z/∂Y_i)，可以各自独立做梯度计算，即只有内存拷贝操作，不需要通信

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
- 如果两层都使用Row Parallel，那么前向传播第一层计算GELU时，每个gpu之间多1次all reduce，第二层输入Scatter
- 如果都使用Column Parallel，第二层的权重会变成[4d,d/4]不能直接和Yi相乘。计算需要完整的Y，即在两个线性层之间多一次all gather，增加显存占用，可能会OOM