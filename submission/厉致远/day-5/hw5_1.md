# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
- 1024 x 1024

#### 1.1.2
- 8 x 128 x 1024

#### 1.1.3
- 8 x 128 x 1024
- 完整的Y通过拼接所有Yi，得到8 x 128 x 4096

### 1.2


#### 1.2.1
- 1024 x 1024

#### 1.2.2
- 8 x 128 x 1024

#### 1.2.3
- 8 x 128 x 1024
- 完整的Z通过求和所有的Zi，得到8 x 128 x 1024

## 2 通信分析

### 2.1

#### 2.1.1
- 需要
- 通信操作是All-gather，收集Yi并拼接
- 3 x 8 x 128 x 1024

#### 2.1.2
- 需要
- 反向传播需要计算L对X的梯度，因此需将所有 rank 上的部分梯度进行All-Reduce求和

### 2.2

#### 2.2.1
- 需要
- 通信操作是All-Reduce
- 3 x 8 x 128 x 1024

#### 2.2.2
- 不需要
- 在前向传播中，Z通过All-Reduce得到，因此在所有rank上是复制的，每个rank使用本地的W2i计算即可

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
- 两层都使用 Row Parallel：额外通信发生在 linear1 和 linear2 之间，需要All-Reduce -> Scatter聚合成完整的Y
- 两层都使用 Colimn Parallel：在 linear1 和 linear2 间必须插入一个All-Gather操作，将所有rank上的Yi收集并拼接